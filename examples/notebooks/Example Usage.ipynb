{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you don't have a virtual environment setup and want to try things quickly, you might want to uncomment and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# pip install -e ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generating Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Print out the mouselab mdp environment registry to see if the environment we're studying is already there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mouselab.envs.registry import registry\n",
    "\n",
    "print(registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### No matching experiment setting?\n",
    "\n",
    "If the experiment setting you want to use is not printed out in the registry of the mouselab package, you have two options:\n",
    " \n",
    "1. Add a new experiment setting to the mouselab package registry (use examples from `mouselab/envs/__init__.py`) and submit a pull request.\n",
    "2. Locally define the experiment setting and add it to the registry -- we will show two examples now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Say we want to investigate an environment with branching factor 1, 1, 1, 1, 3:\n",
    "```\n",
    "\n",
    "        depth  depth   depth   depth    depth\n",
    "start     1      2       3       4        5\n",
    "                                       ()\n",
    "                                     /\n",
    "                                    /\n",
    "                                   /\n",
    " () ---- () ---- () ----() ----- () ---- ()\n",
    "                                    \\\n",
    "                                     \\\n",
    "                                      \\\n",
    "                                       ()\n",
    " ```\n",
    " \n",
    "You have two reward settings you're interested in:\n",
    " \n",
    "- Increasing variance, that is the same as the `large_increasing` environment above.\n",
    "- Decreasing variance, that is the opposite direction of the `large_increasing` environment above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mouselab.envs.registry import register\n",
    "from mouselab.distributions import Normal\n",
    "from mouselab.envs.reward_settings import large_increasing_reward\n",
    "\n",
    "# create narrow_large_increasing\n",
    "register(\n",
    "    name=\"narrow_large_increasing\",\n",
    "    branching=[1,1,1,1,3],\n",
    "    reward_inputs=[\"depth\"],\n",
    "    reward_dictionary=large_increasing_reward,\n",
    ")\n",
    "\n",
    "large_decreasing_reward = {\n",
    "    level_idx + 1: Normal(0, level) for level_idx, level in enumerate([32, 8, 4, 2, 1])\n",
    "}\n",
    "\n",
    "\n",
    "# create narrow_large_decreasing\n",
    "register(\n",
    "    name=\"narrow_large_decreasing\",\n",
    "    branching=[1,1,1,1,3],\n",
    "    reward_inputs=[\"depth\"],\n",
    "    reward_dictionary=large_decreasing_reward,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can now check that your environments are on the registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can get one environment's details using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "registry(\"narrow_large_increasing\").branching, registry(\"narrow_large_increasing\").reward_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now let's instantiate two Mouselab MDP gym environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mouselab.mouselab import MouselabEnv\n",
    "from mouselab.distributions import Categorical\n",
    "\n",
    "\n",
    "env_increasing = MouselabEnv.new_symmetric_registered(\"narrow_large_increasing\",\n",
    "                                           cost=1)\n",
    "env_decreasing = MouselabEnv.new_symmetric_registered(\"narrow_large_decreasing\",\n",
    "                                           cost=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For future steps we will keep using `env_increasing`.\n",
    "\n",
    "Once finished you could print out the current state of the environment:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(env_increasing._state)\n",
    "# Output: ` (0, Norm(0.00, 1.00), Norm(0.00, 2.00), Norm(0.00, 4.00), Norm(0.00, 8.00), Norm(0.00, 32.00), Norm(0.00, 32.00), Norm(0.00, 32.00))`"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now say you see what happens when you reveal the 3rd node:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env_increasing._step(3)\n",
    "print(env_increasing._state)\n",
    "# Output: ` (0, Norm(0.00, 1.00), Norm(0.00, 2.00), <number>, Norm(0.00, 8.00), Norm(0.00, 32.00), Norm(0.00, 32.00), Norm(0.00, 32.00))`"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exact Solution\n",
    "\n",
    "The code for the exact solution can be found in exact.py . There are some limitations on size and distributions (we only want to use categorical distributions, but there is a method to discretize implemented continuous distributions.) Since we are planning over belief states, you should only need to run it once for each reward configuration. The `timed_solve_env` function in `exact_utils.py` is probably good enough depending on what you want as output. \n",
    "\n",
    "If you want to try the solver out on the environment created above:\n",
    "\n",
    "_Note: For the 3-1-2 branching environment, this will probably take around 30 minutes to two hours depending on your machine. Therefore, we use a smaller environment in this example._"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we want to investigate an environment with branching factor 1, 1, 3:\n",
    "```\n",
    "\n",
    "        depth  depth   depth    \n",
    "start     1      2       3           \n",
    "                          ()\n",
    "                         /\n",
    "                       /\n",
    "                     /\n",
    " () ---- () ---- ()  -----()\n",
    "                     \\\n",
    "                      \\\n",
    "                       \\\n",
    "                        ()\n",
    " ```\n",
    "\n",
    "And which shares the reward function of the `high_increasing` environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mouselab.envs.reward_settings import high_increasing_reward\n",
    "\n",
    "register(\n",
    "    name=\"narrow_high_increasing\",\n",
    "    branching=[1,1,3],\n",
    "    reward_inputs=[\"depth\"],\n",
    "    reward_dictionary=high_increasing_reward,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mouselab.exact_utils import timed_solve_env\n",
    "\n",
    "\n",
    "env_increasing = MouselabEnv.new_symmetric_registered(\"narrow_high_increasing\", cost=1)\n",
    "\n",
    "env_increasing.reset()\n",
    "q, v, pi, info = timed_solve_env(env_increasing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The first three outputs are functions (i.e. q(s,a), v(s) and pi(s).)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Saving a solution\n",
    "\n",
    "If you would like to save the Q files, you can save them as a dictionary with keys of (s,a) tuples. Set the `save_q` flag in the `solve` or `timed_solve_env` function to `True`. Please note that solving for the actual Q-values vs solving for the value of an environment can be more costly because we do not (currently) exploit the symmetry between states with a hash function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env_increasing.reset()\n",
    "q, v, pi, info = timed_solve_env(env_increasing, save_q = True)\n",
    "q_dictionary = info[\"q_dictionary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(q_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simulating trajectories\n",
    "\n",
    "To construct trajectories, you need (1) a policy function (found in policies.py); (2) the inputs to that function (one of the functions from the last section); (3) an environment for the simulated agent to be acting on. For example, to simulate a trajectory with a softmax policy:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from mouselab.policies import SoftmaxPolicy\n",
    "from mouselab.agents import Agent\n",
    "\n",
    "agent = Agent()\n",
    "\n",
    "pol = SoftmaxPolicy(q_dictionary)\n",
    "agent.register(pol)\n",
    "\n",
    "agent.register(env_increasing)\n",
    "\n",
    "trace = agent.run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trace"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output will be a dictionary containing information from the simulated episode, including states, actions and rewards. (Check out agents.py for more information and a function for running many episodes.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}